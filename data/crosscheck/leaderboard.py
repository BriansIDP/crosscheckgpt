# Order is TriviaQA, TruthQA M1, Xsum FactKB, 
metrics = {
"mistral": {
    "TriviaQA": 63.89,
    "TruthQA_MC1": 52.51,
    "TruthQA_MC2": 66.82,
    "Xsum_factKB": 55.28,
    "CNN_DM_BERTP": 52.51,
    "MemoTrap": 43.27,
    "FaithDial": 59.08,
    "HaluQA_Acc": 23,
    "HaluSumm": 44.65,
    "HaluDial": 71.53,
},
"vicuna": {
    "TriviaQA": 56.36,
    "TruthQA_MC1": 33.05,
    "TruthQA_MC2": 50.39,
    "Xsum_factKB": 70.86,
    "CNN_DM_BERTP": 43.15,
    "MemoTrap": 42.63,
    "FaithDial": 77.48,
    "HaluQA_Acc": 38.09,
    "HaluSumm": 50.52,
    "HaluDial": 57.74,
},
"llama2": {
    "TriviaQA": 8.83,
    "TruthQA_MC1": 30.23,
    "TruthQA_MC2": 45.31,
    "Xsum_factKB": 47.64,
    "CNN_DM_BERTP": 41.66,
    "MemoTrap": 50.32,
    "FaithDial": 69.74,
    "HaluQA_Acc": 52.31,
    "HaluSumm": 49.06,
    "HaluDial": 64.25,
},
"zephyr": {
    "TriviaQA": 64.43,
    "TruthQA_MC1": 38.68,
    "TruthQA_MC2": 55.12,
    "Xsum_factKB": 59.2,
    "CNN_DM_BERTP": 58.71,
    "MemoTrap": 44.66,
    "FaithDial": 58.75,
    "HaluQA_Acc": 51.85,
    "HaluSumm": 52.38,
    "HaluDial": 76.34,
},
"beluga": {
    "TriviaQA": 9.27,
    "TruthQA_MC1": 34.76,
    "TruthQA_MC2": 50.08,
    "Xsum_factKB": 79.05,
    "CNN_DM_BERTP": 50.24,
    "MemoTrap": 47.44,
    "FaithDial": 82.03,
    "HaluQA_Acc": 29.82,
    "HaluSumm": 40.52,
    "HaluDial": 57,
},
"mistral1": {
    "TriviaQA": 70.86,
    "TruthQA_MC1": 28.15,
    "TruthQA_MC2": 42.63,
    "Xsum_factKB": 31.56,
    "CNN_DM_BERTP": 17.78,
    "MemoTrap": 39.42,
    "FaithDial": 79.17,
    "HaluQA_Acc": 54.48,
    "HaluSumm": 43.13,
    "HaluDial": 59.74,
},
"openorca": {
    "TriviaQA": 65.84,
    "TruthQA_MC1": 35.25,
    "TruthQA_MC2": 52.26,
    "Xsum_factKB": 39.63,
    "CNN_DM_BERTP": 61.51,
    "MemoTrap": 50.53,
    "FaithDial": 82.85,
    "HaluQA_Acc": 44.64,
    "HaluSumm": 51.47,
    "HaluDial": 76.99,
},
"falcon": {
    "TriviaQA": 39.16,
    "TruthQA_MC1": 28.76,
    "TruthQA_MC2": 44.08,
    "Xsum_factKB": 67.47,
    "CNN_DM_BERTP": 45.65,
    "MemoTrap": 61.22,
    "FaithDial": 73.58,
    "HaluQA_Acc": 29.72,
    "HaluSumm": 44.02,
    "HaluDial": 38.55,
},
"starling": {
    "TriviaQA": 65.95,
    "TruthQA_MC1": 31.58,
    "TruthQA_MC2": 47.31,
    "Xsum_factKB": 30.55,
    "CNN_DM_BERTP": 57.09,
    "MemoTrap": 52.78,
    "FaithDial": 62.42,
    "HaluQA_Acc": 59.69,
    "HaluSumm": 54.59,
    "HaluDial": 67.5,
},
"llama2lm": {
    "TriviaQA": 9.22,
    "TruthQA_MC1": 25.21,
    "TruthQA_MC2": 38.97,
    "Xsum_factKB": 80.75,
    "CNN_DM_BERTP": 52.77,
    "MemoTrap": 39.32,
    "FaithDial": 70.36,
    "HaluQA_Acc": 45.66,
    "HaluSumm": 42.79,
    "HaluDial": 49.97,
},
"llama213blm": {
    "TriviaQA": 9.49,
    "TruthQA_MC1": 25.95,
    "TruthQA_MC2": 36.89,
    "Xsum_factKB": 77.28,
    "CNN_DM_BERTP": 37.81,
    "MemoTrap": 35.58,
    "FaithDial": 82.79,
    "HaluQA_Acc": 70.05,
    "HaluSumm": 44.53,
    "HaluDial": 73.93,
}
}
